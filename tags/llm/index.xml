<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on Raj Patil</title><link>https://rajp152k.github.io/tags/llm/</link><description>Recent content in LLM on Raj Patil</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 21 Sep 2023 16:38:07 +0530</lastBuildDate><atom:link href="https://rajp152k.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Prompt Crafting Distilled</title><link>https://rajp152k.github.io/post/dense-guide-prompt-engineering/</link><pubDate>Thu, 21 Sep 2023 16:38:07 +0530</pubDate><guid>https://rajp152k.github.io/post/dense-guide-prompt-engineering/</guid><description>The Premise I was initially reluctant on using generative AI for my writing process.
That being said, I was quite aware of the potential of large language models (generically addressed as LLMs in here henceforth) - especially true in the case of content creators and/or eccentrically curious individuals.
I, therefore, decided to clarify how I&amp;rsquo;ll be using generative AI for my ideation process.
The Promise Before we get onto that, as promised by the title, distilling the over-arching skills needed to extract good insights from a conversation with an LLM (an el-el-em; please don&amp;rsquo;t read it as large, please.</description></item></channel></rss>