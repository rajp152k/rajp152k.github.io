<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Llm on (bit-mage)</title><link>https://rajp152k.github.io/tags/llm/</link><description>Recent content in Llm on (bit-mage)</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 22 Apr 2025 16:12:43 +0530</lastBuildDate><atom:link href="https://rajp152k.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Assume Competence</title><link>https://rajp152k.github.io/post/jargon-ai/</link><pubDate>Tue, 22 Apr 2025 16:12:43 +0530</pubDate><guid>https://rajp152k.github.io/post/jargon-ai/</guid><description>&lt;p>Following a recent realization that &lt;a href="https://rajp152k.github.io/post/phrasal-outlines">jargons are fun&lt;/a>, experimenting with prompts that inform LLMs to talk in outlines and jargons, assuming the reader is competent. Producitvity is up.&lt;/p>
&lt;p>.dotfiles commit for linked context : &lt;a href="https://github.com/rajp152k/.dotfiles/commit/28dd1385cc4370dd0b15774bb96a661b3cab628f">https://github.com/rajp152k/.dotfiles/commit/28dd1385cc4370dd0b15774bb96a661b3cab628f&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>You respond exclusively in highly concise, jargon-rich org-mode only outlines, without any bold or italics formatting: the reader is a competent expert with polymathic knowledge and exceptional contextual comprehension. Do not provide explanations unless asked for further simplifications; instead, communicate with precision and expect the reader to grasp complex concepts and implicit connections immediately. Do not use any filler sentences and collabaratively contribute in constructing whatever topic is being expanded upon&lt;/p></description></item><item><title>I wrote an Emacs Package</title><link>https://rajp152k.github.io/post/fabric-gpt/</link><pubDate>Sun, 16 Mar 2025 19:10:48 +0530</pubDate><guid>https://rajp152k.github.io/post/fabric-gpt/</guid><description>&lt;p>Fabric&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> is a collection of crowd-sourced prompts, exposed via a CLI tool. I used it for a while some time ago but never fully exploited it because I prefer Emacs.&lt;/p>
&lt;p>Eshell buffers are an option, but I am principled in my tool usage and prefer to delegate longer-running CLI tasks to a combination of Alacritty and Tmux.&lt;/p>
&lt;p>Maintaining my Emacs shell usage to ephemeral popups feels natural.&lt;/p>
&lt;p>Gptel&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> is a versatile LLM client that integrates smoothly into my workflow (buffer/text manipulation and management) without disrupting my thought flow.&lt;/p></description></item><item><title>Prompt Crafting Distilled</title><link>https://rajp152k.github.io/post/dense-guide-prompt-engineering/</link><pubDate>Thu, 21 Sep 2023 16:38:07 +0530</pubDate><guid>https://rajp152k.github.io/post/dense-guide-prompt-engineering/</guid><description>&lt;h1 id="the-premise">The Premise&lt;/h1>
&lt;p>I was initially reluctant on using generative AI for my writing
process.&lt;/p>
&lt;p>That being said, I was quite aware of the potential of large language
models (generically addressed as LLMs in here henceforth) -
especially true in the case of content creators and/or eccentrically
curious individuals.&lt;/p>
&lt;p>I, therefore, decided to clarify how I&amp;rsquo;ll be using generative AI for
my ideation process.&lt;/p>
&lt;h1 id="the-promise">The Promise&lt;/h1>
&lt;p>Before we get onto that, as promised by the title, distilling the
over-arching skills needed to extract good insights from a
conversation with an LLM (an el-el-em; please don&amp;rsquo;t read it as large, please..).&lt;/p></description></item></channel></rss>