<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Authorship on (bit-mage)</title><link>https://rajp152k.github.io/tags/authorship/</link><description>Recent content in Authorship on (bit-mage)</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 21 Sep 2023 16:38:07 +0530</lastBuildDate><atom:link href="https://rajp152k.github.io/tags/authorship/index.xml" rel="self" type="application/rss+xml"/><item><title>Prompt Crafting Distilled</title><link>https://rajp152k.github.io/post/dense-guide-prompt-engineering/</link><pubDate>Thu, 21 Sep 2023 16:38:07 +0530</pubDate><guid>https://rajp152k.github.io/post/dense-guide-prompt-engineering/</guid><description>&lt;h1 id="the-premise"&gt;The Premise&lt;/h1&gt;
&lt;p&gt;I was initially reluctant on using generative AI for my writing
process.&lt;/p&gt;
&lt;p&gt;That being said, I was quite aware of the potential of large language
models (generically addressed as LLMs in here henceforth) -
especially true in the case of content creators and/or eccentrically
curious individuals.&lt;/p&gt;
&lt;p&gt;I, therefore, decided to clarify how I&amp;rsquo;ll be using generative AI for
my ideation process.&lt;/p&gt;
&lt;h1 id="the-promise"&gt;The Promise&lt;/h1&gt;
&lt;p&gt;Before we get onto that, as promised by the title, distilling the
over-arching skills needed to extract good insights from a
conversation with an LLM (an el-el-em; please don&amp;rsquo;t read it as large, please..).&lt;/p&gt;</description></item></channel></rss>